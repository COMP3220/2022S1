{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kw312Soxv7-6"
      },
      "source": [
        "# Using word embeddings\n",
        "\n",
        "This notebook is based on the code samples found in Chapter 11 of [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python?a_aid=keras&a_bid=76564dff) and hosted on https://github.com/fchollet/deep-learning-with-python-notebooks. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "z36yJIaJv7-9"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "# tf.config.experimental.list_physical_devices()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2gnnt8hev7--"
      },
      "outputs": [],
      "source": [
        "# physical_devices = tf.config.list_physical_devices('GPU')\n",
        "# tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_qGk7Q_hv7_A",
        "outputId": "5cbd23eb-3157-4adc-8bd3-9b6b8af5cb78"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.8.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "from tensorflow import keras\n",
        "keras.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LK_fBmxTv7_B"
      },
      "source": [
        "A popular and powerful way to associate a vector with a word is the use of dense \"word vectors\", also called \"word embeddings\". \n",
        "While the vectors obtained through one-hot encoding are binary, sparse (mostly made of zeros) and very high-dimensional (same dimensionality as the \n",
        "number of words in the vocabulary), \"word embeddings\" are low-dimensional floating point vectors \n",
        "(i.e. \"dense\" vectors, as opposed to sparse vectors). \n",
        "Unlike word vectors obtained via one-hot encoding, word embeddings are learned from data. \n",
        "It is common to see word embeddings that are 256-dimensional, 512-dimensional, or 1024-dimensional when dealing with very large vocabularies. \n",
        "On the other hand, one-hot encoding words generally leads to vectors that are 20,000-dimensional or higher (capturing a vocabulary of 20,000 \n",
        "token in this case). So, word embeddings pack more information into far fewer dimensions. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhBOS9Cvv7_C"
      },
      "source": [
        "![word embeddings vs. one hot encoding](https://s3.amazonaws.com/book.keras.io/img/ch6/word_embeddings.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2OIFGc5v7_D"
      },
      "source": [
        "There are two ways to obtain word embeddings:\n",
        "\n",
        "1. Learn word embeddings jointly with the main task you care about (e.g. document classification or sentiment prediction). \n",
        "In this setup, you would start with random word vectors, then learn your word vectors in the same way that you learn the weights of a neural network.\n",
        "2. Load into your model word embeddings that were pre-computed using a different machine learning task than the one you are trying to solve. \n",
        "These are called \"pre-trained word embeddings\". "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmzYXFYFv7_D"
      },
      "source": [
        "## Learning word embeddings with the `Embedding` layer\n",
        "\n",
        "\n",
        "* The simplest way to associate a dense vector to a word would be to pick the vector at random and let the model learn the best values of the vector during the training stage.\n",
        "* Keras provides the `Embedding` layer that facilitates this.\n",
        "* The `Embedding` layer is normally the first layer of the neural network.\n",
        "* The `Embedding` layer takes a word index as input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "WLM-Vsfav7_E"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "# The Embedding layer takes at least two arguments:\n",
        "# the number of possible tokens, here 1000 (1 + maximum word index),\n",
        "# and the dimensionality of the embeddings, here 64.\n",
        "embedding_layer = Embedding(1000, 64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWV3dwtbv7_H"
      },
      "source": [
        "\n",
        "The `Embedding` layer is best understood as a dictionary mapping integer indices (which stand for specific words) to dense vectors. It takes \n",
        "as input integers, it looks up these integers into an internal dictionary, and it returns the associated vectors. It's effectively a dictionary lookup."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbS6FpYpv7_H"
      },
      "source": [
        "\n",
        "The `Embedding` layer takes as input a 2D tensor of integers, of shape `(samples, sequence_length)`, where each entry is a sequence of \n",
        "integers. It can embed sequences of variable lengths, so for instance we could feed into our embedding layer above batches that could have \n",
        "shapes `(32, 10)` (batch of 32 sequences of length 10) or `(64, 15)` (batch of 64 sequences of length 15). All sequences in a batch must \n",
        "have the same length, though (since we need to pack them into a single tensor), so sequences that are shorter than others should be padded \n",
        "with zeros, and sequences that are longer should be truncated.\n",
        "\n",
        "This layer returns a 3D floating point tensor, of shape `(samples, sequence_length, embedding_dimensionality)`. Such a 3D tensor can then \n",
        "be processed by a RNN layer or a 1D convolution layer (both will be introduced in the next sections).\n",
        "\n",
        "When you instantiate an `Embedding` layer, its weights (its internal dictionary of token vectors) are initially random, just like with any \n",
        "other layer. During training, these word vectors will be gradually adjusted via backpropagation, structuring the space into something that the \n",
        "downstream model can exploit. Once fully trained, your embedding space will show a lot of structure -- a kind of structure specialized for \n",
        "the specific problem you were training your model for."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5OvqvYFv7_J"
      },
      "source": [
        "Keras' `pad_sequences` converts a sequence of lists of word indices into a matrix of rows so that:\n",
        "* If a sequence is longer than the maximum length, the sequence is truncated (by default at the beginning).\n",
        "* If a sequence is shorter than the maximum length, zeros are padded (by default at the beginning)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z94NDtRnv7_J",
        "outputId": "39e88836-17a6-48ec-be82-6be95bfb8d51"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0,  0,  1,  2, 23, 43],\n",
              "       [ 6,  1, 31,  3,  4, 21]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "from tensorflow.keras import preprocessing\n",
        "my_data = [[1,2,23,43], [2,6,1,31,3,4,21]]\n",
        "preprocessing.sequence.pad_sequences(my_data, maxlen=6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drS9FFigv7_K"
      },
      "source": [
        "Let's apply this idea to the IMDB movie review sentiment prediction task that you are already familiar with. Let's quickly prepare \n",
        "the data. We will restrict the movie reviews to the top 10,000 most common words (like we did the first time we worked with this dataset), \n",
        "and cut the reviews after only 20 words. Our network will simply learn 8-dimensional embeddings for each of the 10,000 words, turn the \n",
        "input integer sequences (2D integer tensor) into embedded sequences (3D float tensor), flatten the tensor to 2D, and train a single `Dense` \n",
        "layer on top for classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HgIVG4bCv7_L",
        "outputId": "a8cd00d2-7e77-46fb-96eb-8c42ae7519de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n",
            "17473536/17464789 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras import preprocessing\n",
        "\n",
        "# Number of words to consider as features\n",
        "max_features = 10000\n",
        "# Cut texts after this number of words \n",
        "# (among top max_features most common words)\n",
        "maxlen = 20\n",
        "\n",
        "# Load the data as lists of integers.\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "\n",
        "# This turns our lists of integers\n",
        "# into a 2D integer tensor of shape `(samples, maxlen)`\n",
        "x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "dBkLwF2cv7_L"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Dense\n",
        "\n",
        "model = Sequential()\n",
        "# We specify the maximum input length to our Embedding layer\n",
        "# so we can later flatten the embedded inputs\n",
        "model.add(Embedding(10000, 8, input_length=maxlen))\n",
        "# After the Embedding layer, \n",
        "# our activations have shape `(samples, maxlen, 8)`.\n",
        "\n",
        "# We flatten the 3D tensor of embeddings \n",
        "# into a 2D tensor of shape `(samples, maxlen * 8)`\n",
        "model.add(Flatten())\n",
        "\n",
        "# We add the classifier on top\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdaNYbLxv7_M",
        "outputId": "d62ef077-fa38-4b86-eda4-289f800d615e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 20, 8)             80000     \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 160)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 161       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 80,161\n",
            "Trainable params: 80,161\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbjAPJJkv7_N"
      },
      "source": [
        "The following code trains the model using 10 epochs and a batch size of 32. Also, prior to training the model, it partitions the data into a training set and a validation set. A validation split of 0.2 indicates that 20% of the data set is used for the validation set.\n",
        "\n",
        "Keras will allocate the first samples of the data set to the training set, and the final samples to the validation set. If you want to do a random partition of the data set, you should shuffle the data **before** calling to `fit`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfSXzHTYv7_N",
        "outputId": "58782da0-0a90-4df7-c91b-734ebb7c12ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "625/625 [==============================] - 3s 3ms/step - loss: 0.6627 - acc: 0.6320 - val_loss: 0.6035 - val_acc: 0.7072\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.5313 - acc: 0.7552 - val_loss: 0.5204 - val_acc: 0.7344\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.4570 - acc: 0.7887 - val_loss: 0.4982 - val_acc: 0.7466\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.4187 - acc: 0.8101 - val_loss: 0.4922 - val_acc: 0.7498\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.3921 - acc: 0.8259 - val_loss: 0.4924 - val_acc: 0.7532\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.3705 - acc: 0.8378 - val_loss: 0.4951 - val_acc: 0.7538\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.3513 - acc: 0.8497 - val_loss: 0.5008 - val_acc: 0.7560\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.3335 - acc: 0.8589 - val_loss: 0.5063 - val_acc: 0.7550\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.3168 - acc: 0.8675 - val_loss: 0.5147 - val_acc: 0.7508\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.3013 - acc: 0.8760 - val_loss: 0.5236 - val_acc: 0.7498\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(x_train, y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=32,\n",
        "                    validation_split=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "BisYZQCRv7_O",
        "outputId": "79d82530-54d9-43b6-99d4-0b633284d394"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.subplot(121)\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "#plt.figure()\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6ein2KXv7_O"
      },
      "source": [
        "We get to a validation accuracy of ~76%, which is pretty good considering that we only look at the first 20 words in every review. But \n",
        "note that merely flattening the embedded sequences and training a single `Dense` layer on top leads to a model that treats each word in the \n",
        "input sequence separately, without considering inter-word relationships and sentence structure (e.g. it would likely treat both _\"this movie \n",
        "is shit\"_ and _\"this movie is the shit\"_ as being negative \"reviews\"). The following alternatives would normally give better results:\n",
        "\n",
        "1. Average the word embeddings to generate a summary of the word embeddings. Keras has the layer `GlobalAveragePooling()` that can be used, for example:\n",
        "\n",
        "```\n",
        "model = Sequential()\n",
        "model.add(Embedding(10000, 8, input_length=maxlen))\n",
        "# After the Embedding layer, \n",
        "# our activations have shape `(samples, maxlen, 8)`.\n",
        "\n",
        "model.add(GlobalAveragePooling1D())\n",
        "# After computing the average, \n",
        "# our activations have shape `(samples, 8)`\n",
        "\n",
        "# We add the classifier on top\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "```\n",
        "\n",
        "2. Add a recurrent layer (we will cover this later in the course).\n",
        "\n",
        "3. Add a 1D convolutional layer (see the textbook for details).\n",
        "\n",
        "Option 1 is a quick solution that sometimes gives surprisingly good results. Options 2 and 3 are more complex solutions that learn word dependencies in the input text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdQf-Tpnv7_P"
      },
      "source": [
        "## Using pre-trained word embeddings\n",
        "\n",
        "\n",
        "Sometimes, you have so little training data available that could never use your data alone to learn an appropriate task-specific embedding \n",
        "of your vocabulary. What to do then?\n",
        "\n",
        "We can then use **pre-trained word embeddings**, created by a third party!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQG2gNMvv7_P"
      },
      "source": [
        "Instead of learning word embeddings jointly with the problem you want to solve, you could be loading embedding vectors from a pre-computed \n",
        "embedding space known to be highly structured and to exhibit useful properties -- that captures generic aspects of language structure. The \n",
        "rationale behind using pre-trained word embeddings in natural language processing is very much the same as for using pre-trained convnets \n",
        "in image classification: we don't have enough data available to learn truly powerful features on our own, but we expect the features that \n",
        "we need to be fairly generic, i.e. common visual features or semantic features. In this case it makes sense to reuse features learned on a \n",
        "different problem.\n",
        "\n",
        "Such word embeddings are generally computed using word occurrence statistics (observations about what words co-occur in sentences or \n",
        "documents), using a variety of techniques, some involving neural networks, others not. The idea of a dense, low-dimensional embedding space \n",
        "for words, computed in an unsupervised way, was initially explored by Bengio et al. in the early 2000s, but it only started really taking \n",
        "off in research and industry applications after the release of one of the most famous and successful word embedding scheme: the Word2Vec \n",
        "algorithm, developed by Mikolov at Google in 2013. Word2Vec dimensions capture specific semantic properties, e.g. gender.\n",
        "\n",
        "There are various pre-computed databases of word embeddings that can download and start using in a Keras `Embedding` layer. Word2Vec is one \n",
        "of them. Another popular one is called \"GloVe\", developed by Stanford researchers in 2014. It stands for \"Global Vectors for Word \n",
        "Representation\", and it is an embedding technique based on factorizing a matrix of word co-occurrence statistics. Its developers have made \n",
        "available pre-computed embeddings for millions of English tokens, obtained from Wikipedia data or from Common Crawl data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0Yc8pXNv7_P"
      },
      "source": [
        "## Putting it all together: from raw text to word embeddings\n",
        "\n",
        "\n",
        "Let's take a look at how you can get started using GloVe embeddings in a Keras model. The same method will of course be valid for Word2Vec \n",
        "embeddings or any other word embedding database that you can download. **We will also use this example to introduce Keras' text tokenization \n",
        "techniques.**\n",
        "\n",
        "We will be using a model similar to the one we just went over -- embedding sentences in sequences of vectors, flattening them and training a \n",
        "`Dense` layer on top. But we will do it using pre-trained word embeddings, and instead of using the pre-tokenized IMDB data packaged in \n",
        "Keras, we will start from scratch, by downloading the original text data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcKPrLhiv7_Q"
      },
      "source": [
        "### Download the IMDB data as raw text\n",
        "\n",
        "\n",
        "First, head to `http://ai.stanford.edu/~amaas/data/sentiment/` and download the raw IMDB dataset (if the URL isn't working anymore, just \n",
        "Google \"IMDB dataset\"). Uncompress it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "APgQsiKev7_Q"
      },
      "outputs": [],
      "source": [
        "#! wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "\n",
        "import requests\n",
        "\n",
        "url = \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
        "filename = url.split(\"/\")[-1]\n",
        "with open(filename, \"wb\") as f:\n",
        "    r = requests.get(url)\n",
        "    f.write(r.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Q7npLKUxv7_R"
      },
      "outputs": [],
      "source": [
        "!tar xzf aclImdb_v1.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNwtsenkv7_R",
        "outputId": "0a52bac7-8873-46c8-a226-ef4d60da8b83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "imdbEr.txt  imdb.vocab\tREADME\ttest  train\n"
          ]
        }
      ],
      "source": [
        "!dir aclImdb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!dir aclImdb/train"
      ],
      "metadata": {
        "id": "q3iG7U7p80eN",
        "outputId": "c5280e3d-d79d-429d-88bd-c16ef0560bcb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "labeledBow.feat  pos\tunsupBow.feat  urls_pos.txt\n",
            "neg\t\t unsup\turls_neg.txt   urls_unsup.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xmxaYKiv7_R"
      },
      "source": [
        "Now let's collect the individual training reviews into a list of strings, one string per review, and let's also collect the review labels \n",
        "(positive / negative) into a `labels` list:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "88XBZXdsv7_S"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "imdb_dir = 'aclImdb'\n",
        "train_dir = os.path.join(imdb_dir, 'train')\n",
        "\n",
        "labels = []\n",
        "texts = []\n",
        "\n",
        "for label_type in ['neg', 'pos']:\n",
        "    dir_name = os.path.join(train_dir, label_type)\n",
        "    for fname in os.listdir(dir_name):\n",
        "        if fname[-4:] == '.txt':\n",
        "            f = open(os.path.join(dir_name, fname), encoding=\"utf-8\")\n",
        "            texts.append(f.read())\n",
        "            f.close()\n",
        "            if label_type == 'neg':\n",
        "                labels.append(0)\n",
        "            else:\n",
        "                labels.append(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "a9GmtLu3v7_S",
        "outputId": "9943675e-92db-4677-cabd-1ad0decbd53b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'One of the most common entries in the \\'goofs\\' category is anachronism. Though I\\'m beginning to believe that anachronism and other goofs are more acceptable, even ignored, in very good films, but are found front and center in rotten films. KISS THEM FOR ME is a rotten film and reeks of anachronism, yet when watching it closely, I found almost nothing specifically anachronistic.<br /><br />The shots of aircraft which bookend the film are certainly out of place. The big 4 engine transport seen after the title \"Honolulu 1944\" appears to be the post war C-97 Stratofreighter (in MATS colors). The combat planes seen taking off from the carrier at the end are Douglas Skyraiders which entered service after WW2 and were made famous by their service in Vietnam.<br /><br />But excepting these two pieces of film and, of course, the hairstyles, everything else is very possibly period authentic. It just \\'feels\\' so wrong. I\\'m an admirer of Stanley Donen, we share the same birthday. In his co directed ON THE TOWN (1949) there is a car chase at the end with the police driving 1949/50 Ford\\'s yet there isn\\'t the slightest feeling that this is out of place in a WW2 period film. In fact, as I reflected later, there isn\\'t anything which says that this is supposed to be a WW2 period film. It just feels that way. Based on a wartime Broadway musical which was based on a ballet (Fancy Free) which may have been based on the work of artist Paul Cadmus (The Fleet\\'s In! 1934) its a great film about sailors on a 24 hour pass in New York and, so heavy with wartime associations, its merely assumed it takes place during the war and yet these contemporary cars do nothing to break the spell.<br /><br />The first problem is old Cary Grant. Though far too old to represent a Navy SBD dive bomber pilot, it is a Hollywood tradition for stars like Grant, Gary Cooper (Lou Gehrig), Jimmy Stewart (Charles Lindbergh) to play younger. It was the role which he is miscast in, not his age. He plays an operator, as they used to call them. A guy who gets things done and breaks all the rules while doing it yet remains admired and loved for it. A hustler. A wheeler dealer. A de rigueur character in a service comedy. Grant is the comic center of what is after all supposed to be a service comedy which is contra to his comedy style.<br /><br />Thinking back on the great Grant comic performances like BRINGING UP BABY (1938) or ARSENIC AND OLD LACE (1944) and he is the great reactor whose comedy is to be reduced by his context from dignity to a befuddled puddle of inert jelly. IN KISS THEM he is expected to be the comic spark plug which just isn\\'t him. People had already been exposed to the type, most recently to comic Phil Silvers as Sgt. Bilko on television. The role would be perfected later by James Garner but here Grant just isn\\'t funny and appears to be a bully getting his way by aggressively pushing his Cary Grantness rather than cajoling and finessing.<br /><br />But the thing which really stinks the place up with anachronism is the lead women. There can be no more echt 50s women than Suzy Parker and Jayne Mansfield. They are unique to the decade. Marilyn Monroe can be placed in a continuum with Carole Lombard and Marie Wilson and any number of dumb blonds, and Grace Kelly was another high class dame (think of Mary Astor), but there never could have been an anatomically exaggerated woman in films like Mansfield. Sure there were the \\'sweater girls\\' (e.g. Lana Turner) of WW2, but Mansfield was stretching the point. Suzy Parker was THE model who revolutionized the model business, who changed the mannequin like poses to become the first natural girl who moved and whose personality was captured by the camera (see FUNNY FACE (1957) also by Stanley Donen).<br /><br />Of course in high 50s style, there seems to be a lot of gender mixing at \\'wild\\' parties but never even a hint of sex (think of the 50s TV shows Bachelor Father or The Bob Cummings Show where dinner jacketed men returned from \\'dates\\' alone). The original book, which I haven\\'t read, was published during the war and appeared as a play on Broadway at the end of the war and the nuances of the situation must have been inescapable for contemporary readers and audiences, but broken down, bowdlerized and reconstituted a dozen years later and fatally miscast, it remains a once forgotten stain on otherwise exemplary careers until the invention of the VCR and cable television resurrected this petrified turkey.<br /><br />So the lesson here is whatever the \\'goof\\' it will be ignored in a great film like CITIZEN KANE (who actually hears Charles Foster Kane say \\'Rosebud\\'?), and tolerated in fun dreck like WESTWORLD ( why were the robots given live ammunition in the first place?) but absolutely despised in a rotten film, even if the goofs are really non existent.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "texts[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8eHRcBHv7_T",
        "outputId": "7b69015e-7648-434d-bf72-319123c1797b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "labels[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tu235z7lv7_T"
      },
      "source": [
        "### Tokenize the data\n",
        "\n",
        "Keras' tokenizer can be used to map each word to a word index through the following two steps:\n",
        "\n",
        "1. Create a mapping of words to indices using `fit_on_texts` on the training data.\n",
        "2. Find the indices of the training data using `texts_to_sequences`.\n",
        "\n",
        "Let's vectorize the texts we collected, and prepare a training and validation split.\n",
        "We will merely be using the concepts we introduced earlier in this section.\n",
        "\n",
        "Because pre-trained word embeddings are meant to be particularly useful on problems where little training data is available (otherwise, \n",
        "task-specific embeddings are likely to outperform them), **we will add the following twist: we restrict the training data to its first 200 \n",
        "samples**. So we will be learning to classify movie reviews after looking at just 200 examples...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBX3LTW3v7_U",
        "outputId": "be1c6aec-e57b-4f1d-e6e7-ba8ed0dfcb45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 88583 unique tokens.\n",
            "Shape of data tensor: (25000, 100)\n",
            "Shape of label tensor: (25000,)\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "maxlen = 100  # We will cut reviews after 100 words\n",
        "training_samples = 200  # We will be training on 200 samples\n",
        "validation_samples = 10000  # We will be validating on 10000 samples\n",
        "max_words = 10000  # We will only consider the top 10,000 words in the dataset\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words, oov_token=\"[UNK]\")\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "data = pad_sequences(sequences, maxlen=maxlen)\n",
        "\n",
        "labels = np.asarray(labels)\n",
        "print('Shape of data tensor:', data.shape)\n",
        "print('Shape of label tensor:', labels.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's double-check that the system tokenises well by finding the word indices and then back to the text. We will use text that includes unknown words."
      ],
      "metadata": {
        "id": "D466YBmS9zsb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inverted_word_index = dict((i, w) for w, i in tokenizer.word_index.items())"
      ],
      "metadata": {
        "id": "PU3cmTKh9-dI"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_indices = tokenizer.texts_to_sequences([\"This is an example sentence with unknwn words\"])\n",
        "sample_indices"
      ],
      "metadata": {
        "id": "rqxD4RVR-X5e",
        "outputId": "cbbaaf56-6647-4f00-fbf8-0fa72fb5893b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[12, 7, 33, 461, 4127, 17, 1, 713]]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\" \".join([inverted_word_index[x] for x in sample_indices[0]])"
      ],
      "metadata": {
        "id": "L3fBneSc-97a",
        "outputId": "d6e54985-8e8e-4a86-c706-2d55a9c603b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'this is an example sentence with [UNK] words'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jsp58EGgv7_V"
      },
      "source": [
        "Incidentally, you can also use Keras' tokeniser to generate one-hot encoding by using `texts_to_matrix` with the `mode=binary` option. Look at Keras documentation for other encoding options: https://faroit.github.io/keras-docs/1.2.2/preprocessing/text/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zl0ls1Drv7_V",
        "outputId": "647c31ca-3036-4c1a-c998-4fc66bbd3e83"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 10000)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "one_hot = tokenizer.texts_to_matrix(texts, mode='binary')\n",
        "one_hot.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "96hjs71Lv7_V"
      },
      "outputs": [],
      "source": [
        "# Split the data into a training set and a validation set\n",
        "# But first, shuffle the data, since we started from data\n",
        "# where samples are ordered (all negative first, then all positive).\n",
        "indices = np.arange(data.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "data = data[indices]\n",
        "labels = labels[indices]\n",
        "\n",
        "x_train = data[:training_samples]\n",
        "y_train = labels[:training_samples]\n",
        "x_val = data[training_samples: training_samples + validation_samples]\n",
        "y_val = labels[training_samples: training_samples + validation_samples]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bk7guN0uv7_V"
      },
      "source": [
        "### Download the GloVe word embeddings\n",
        "\n",
        "\n",
        "Head to `https://nlp.stanford.edu/projects/glove/` (where you can learn more about the GloVe algorithm), and download the pre-computed \n",
        "embeddings from 2014 English Wikipedia. It's a 822MB zip file named `glove.6B.zip`, containing 100-dimensional embedding vectors for \n",
        "400,000 words (or non-word tokens). Un-zip it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "yapo8u6Tv7_W"
      },
      "outputs": [],
      "source": [
        "# !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "    \n",
        "url = \"http://nlp.stanford.edu/data/glove.6B.zip\"\n",
        "filename = url.split(\"/\")[-1]\n",
        "with open(filename, \"wb\") as f:\n",
        "    r = requests.get(url)\n",
        "    f.write(r.content)    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qb-CJAiIv7_W"
      },
      "source": [
        "The following code unzips the data. If you do not have unzip installed or you are using Goocle Colaboratory you may need to run this first:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "LXQ81G3Av7_W"
      },
      "outputs": [],
      "source": [
        "#!apt install unzip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oh5FPpIbv7_W",
        "outputId": "d31530c2-20a9-440d-9ea9-f925c972e226"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ]
        }
      ],
      "source": [
        "!unzip glove.6B.zip\n",
        "#!tar xzf glove.6B.zip # Use this instead if you are using a Windows machine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPQ4ym-0v7_W"
      },
      "source": [
        "### Alternatively, map Google drive (if you are using Google Colaboratory)\n",
        "\n",
        "If you are using [Google colaboratory](https://colab.research.google.com) you can use cloud instances with a GPU and you can also store data in your Google Drive. The following cells of code mount your Google Drive after following an authorisation step. Uncomment them and run them on a Google colaboratory notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uYX60OOov7_X"
      },
      "outputs": [],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7F73Fzov7_X"
      },
      "outputs": [],
      "source": [
        "#cp '/gdrive/My Drive/COMP348/glove/glove.6B.100d.txt' ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Me_s3b4Pv7_X"
      },
      "source": [
        "### Pre-process the embeddings\n",
        "\n",
        "\n",
        "Let's parse the un-zipped file to build an index mapping words (as strings) to their vector representation (as number vectors). The file is a text file where each line shows the word followed by the vector representation. For example, the first lines of `glove.6B.50d.txt` are:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5aLQsClZv7_Y",
        "outputId": "80feb800-1609-4895-e9d4-160b429dfbd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the -0.038194 -0.24487 0.72812 -0.39961 0.083172 0.043953 -0.39141 0.3344 -0.57545 0.087459 0.28787 -0.06731 0.30906 -0.26384 -0.13231 -0.20757 0.33395 -0.33848 -0.31743 -0.48336 0.1464 -0.37304 0.34577 0.052041 0.44946 -0.46971 0.02628 -0.54155 -0.15518 -0.14107 -0.039722 0.28277 0.14393 0.23464 -0.31021 0.086173 0.20397 0.52624 0.17164 -0.082378 -0.71787 -0.41531 0.20335 -0.12763 0.41367 0.55187 0.57908 -0.33477 -0.36559 -0.54857 -0.062892 0.26584 0.30205 0.99775 -0.80481 -3.0243 0.01254 -0.36942 2.2167 0.72201 -0.24978 0.92136 0.034514 0.46745 1.1079 -0.19358 -0.074575 0.23353 -0.052062 -0.22044 0.057162 -0.15806 -0.30798 -0.41625 0.37972 0.15006 -0.53212 -0.2055 -1.2526 0.071624 0.70565 0.49744 -0.42063 0.26148 -1.538 -0.30223 -0.073438 -0.28312 0.37104 -0.25217 0.016215 -0.017099 -0.38984 0.87424 -0.72569 -0.51058 -0.52028 -0.1459 0.8278 0.27062\n",
            ", -0.10767 0.11053 0.59812 -0.54361 0.67396 0.10663 0.038867 0.35481 0.06351 -0.094189 0.15786 -0.81665 0.14172 0.21939 0.58505 -0.52158 0.22783 -0.16642 -0.68228 0.3587 0.42568 0.19021 0.91963 0.57555 0.46185 0.42363 -0.095399 -0.42749 -0.16567 -0.056842 -0.29595 0.26037 -0.26606 -0.070404 -0.27662 0.15821 0.69825 0.43081 0.27952 -0.45437 -0.33801 -0.58184 0.22364 -0.5778 -0.26862 -0.20425 0.56394 -0.58524 -0.14365 -0.64218 0.0054697 -0.35248 0.16162 1.1796 -0.47674 -2.7553 -0.1321 -0.047729 1.0655 1.1034 -0.2208 0.18669 0.13177 0.15117 0.7131 -0.35215 0.91348 0.61783 0.70992 0.23955 -0.14571 -0.37859 -0.045959 -0.47368 0.2385 0.20536 -0.18996 0.32507 -1.1112 -0.36341 0.98679 -0.084776 -0.54008 0.11726 -1.0194 -0.24424 0.12771 0.013884 0.080374 -0.35414 0.34951 -0.7226 0.37549 0.4441 -0.99059 0.61214 -0.35111 -0.83155 0.45293 0.082577\n",
            ". -0.33979 0.20941 0.46348 -0.64792 -0.38377 0.038034 0.17127 0.15978 0.46619 -0.019169 0.41479 -0.34349 0.26872 0.04464 0.42131 -0.41032 0.15459 0.022239 -0.64653 0.25256 0.043136 -0.19445 0.46516 0.45651 0.68588 0.091295 0.21875 -0.70351 0.16785 -0.35079 -0.12634 0.66384 -0.2582 0.036542 -0.13605 0.40253 0.14289 0.38132 -0.12283 -0.45886 -0.25282 -0.30432 -0.11215 -0.26182 -0.22482 -0.44554 0.2991 -0.85612 -0.14503 -0.49086 0.0082973 -0.17491 0.27524 1.4401 -0.21239 -2.8435 -0.27958 -0.45722 1.6386 0.78808 -0.55262 0.65 0.086426 0.39012 1.0632 -0.35379 0.48328 0.346 0.84174 0.098707 -0.24213 -0.27053 0.045287 -0.40147 0.11395 0.0062226 0.036673 0.018518 -1.0213 -0.20806 0.64072 -0.068763 -0.58635 0.33476 -1.1432 -0.1148 -0.25091 -0.45907 -0.096819 -0.17946 -0.063351 -0.67412 -0.068895 0.53604 -0.87773 0.31802 -0.39242 -0.23394 0.47298 -0.028803\n",
            "of -0.1529 -0.24279 0.89837 0.16996 0.53516 0.48784 -0.58826 -0.17982 -1.3581 0.42541 0.15377 0.24215 0.13474 0.41193 0.67043 -0.56418 0.42985 -0.012183 -0.11677 0.31781 0.054177 -0.054273 0.35516 -0.30241 0.31434 -0.33846 0.71715 -0.26855 -0.15837 -0.47467 0.051581 -0.33252 0.15003 -0.1299 -0.54617 -0.37843 0.64261 0.82187 -0.080006 0.078479 -0.96976 -0.57741 0.56491 -0.39873 -0.057099 0.19743 0.065706 -0.48092 -0.20125 -0.40834 0.39456 -0.02642 -0.11838 1.012 -0.53171 -2.7474 -0.042981 -0.74849 1.7574 0.59085 0.04885 0.78267 0.38497 0.42097 0.67882 0.10337 0.6328 -0.026595 0.58647 -0.44332 0.33057 -0.12022 -0.55645 0.073611 0.20915 0.43395 -0.012761 0.089874 -1.7991 0.084808 0.77112 0.63105 -0.90685 0.60326 -1.7515 0.18596 -0.50687 -0.70203 0.66578 -0.81304 0.18712 -0.018488 -0.26757 0.727 -0.59363 -0.34839 -0.56094 -0.591 1.0039 0.20664\n",
            "to -0.1897 0.050024 0.19084 -0.049184 -0.089737 0.21006 -0.54952 0.098377 -0.20135 0.34241 -0.092677 0.161 -0.13268 -0.2816 0.18737 -0.42959 0.96039 0.13972 -1.0781 0.40518 0.50539 -0.55064 0.4844 0.38044 -0.0029055 -0.34942 -0.099696 -0.78368 1.0363 -0.2314 -0.47121 0.57126 -0.21454 0.35958 -0.48319 1.0875 0.28524 0.12447 -0.039248 -0.076732 -0.76343 -0.32409 -0.5749 -1.0893 -0.41811 0.4512 0.12112 -0.51367 -0.13349 -1.1378 -0.28768 0.16774 0.55804 1.5387 0.018859 -2.9721 -0.24216 -0.92495 2.1992 0.28234 -0.3478 0.51621 -0.43387 0.36852 0.74573 0.072102 0.27931 0.92569 -0.050336 -0.85856 -0.1358 -0.92551 -0.33991 -1.0394 -0.067203 -0.21379 -0.4769 0.21377 -0.84008 0.052536 0.59298 0.29604 -0.67644 0.13916 -1.5504 -0.20765 0.7222 0.52056 -0.076221 -0.15194 -0.13134 0.058617 -0.31869 -0.61419 -0.62393 -0.41548 -0.038175 -0.39804 0.47647 -0.15983\n",
            "and -0.071953 0.23127 0.023731 -0.50638 0.33923 0.1959 -0.32943 0.18364 -0.18057 0.28963 0.20448 -0.5496 0.27399 0.58327 0.20468 -0.49228 0.19974 -0.070237 -0.88049 0.29485 0.14071 -0.1009 0.99449 0.36973 0.44554 0.28998 -0.1376 -0.56365 -0.029365 -0.4122 -0.25269 0.63181 -0.44767 0.24363 -0.10813 0.25164 0.46967 0.3755 -0.23613 -0.14129 -0.44537 -0.65737 -0.042421 -0.28636 -0.28811 0.063766 0.20281 -0.53542 0.41307 -0.59722 -0.38614 0.19389 -0.17809 1.6618 -0.011819 -2.3737 0.058427 -0.2698 1.2823 0.81925 -0.22322 0.72932 -0.053211 0.43507 0.85011 -0.42935 0.92664 0.39051 1.0585 -0.24561 -0.18265 -0.5328 0.059518 -0.66019 0.18991 0.28836 -0.2434 0.52784 -0.65762 -0.14081 1.0491 0.5134 -0.23816 0.69895 -1.4813 -0.2487 -0.17936 -0.059137 -0.08056 -0.48782 0.014487 -0.6259 -0.32367 0.41862 -1.0807 0.46742 -0.49931 -0.71895 0.86894 0.19539\n",
            "in 0.085703 -0.22201 0.16569 0.13373 0.38239 0.35401 0.01287 0.22461 -0.43817 0.50164 -0.35874 -0.34983 0.055156 0.69648 -0.17958 0.067926 0.39101 0.16039 -0.26635 -0.21138 0.53698 0.49379 0.9366 0.66902 0.21793 -0.46642 0.22383 -0.36204 -0.17656 0.1748 -0.20367 0.13931 0.019832 -0.10413 -0.20244 0.55003 -0.1546 0.98655 -0.26863 -0.2909 -0.32866 -0.34188 -0.16943 -0.42001 -0.046727 -0.16327 0.70824 -0.74911 -0.091559 -0.96178 -0.19747 0.10282 0.55221 1.3816 -0.65636 -3.2502 -0.31556 -1.2055 1.7709 0.4026 -0.79827 1.1597 -0.33042 0.31382 0.77386 0.22595 0.52471 -0.034053 0.32048 0.079948 0.17752 -0.49426 -0.70045 -0.44569 0.17244 0.20278 0.023292 -0.20677 -1.0158 0.18325 0.56752 0.31821 -0.65011 0.68277 -0.86585 -0.059392 -0.29264 -0.55668 -0.34705 -0.32895 0.40215 -0.12746 -0.20228 0.87368 -0.545 0.79205 -0.20695 -0.074273 0.75808 -0.34243\n",
            "a -0.27086 0.044006 -0.02026 -0.17395 0.6444 0.71213 0.3551 0.47138 -0.29637 0.54427 -0.72294 -0.0047612 0.040611 0.043236 0.29729 0.10725 0.40156 -0.53662 0.033382 0.067396 0.64556 -0.085523 0.14103 0.094539 0.74947 -0.194 -0.68739 -0.41741 -0.22807 0.12 -0.48999 0.80945 0.045138 -0.11898 0.20161 0.39276 -0.20121 0.31354 0.75304 0.25907 -0.11566 -0.029319 0.93499 -0.36067 0.5242 0.23706 0.52715 0.22869 -0.51958 -0.79349 -0.20368 -0.50187 0.18748 0.94282 -0.44834 -3.6792 0.044183 -0.26751 2.1997 0.241 -0.033425 0.69553 -0.64472 -0.0072277 0.89575 0.20015 0.46493 0.61933 -0.1066 0.08691 -0.4623 0.18262 -0.15849 0.020791 0.19373 0.063426 -0.31673 -0.48177 -1.3848 0.13669 0.96859 0.049965 -0.2738 -0.035686 -1.0577 -0.24467 0.90366 -0.12442 0.080776 -0.83401 0.57201 0.088945 -0.42532 -0.018253 -0.079995 -0.28581 -0.01089 -0.4923 0.63687 0.23642\n",
            "\" -0.30457 -0.23645 0.17576 -0.72854 -0.28343 -0.2564 0.26587 0.025309 -0.074775 -0.3766 -0.057774 0.12159 0.34384 0.41928 -0.23236 -0.31547 0.60939 0.25117 -0.68667 0.70873 1.2162 -0.1824 -0.48442 -0.33445 0.30343 1.086 0.49992 -0.20198 0.27959 0.68352 -0.33566 -0.12405 0.059656 0.33617 0.37501 0.56552 0.44867 0.11284 -0.16196 -0.94346 -0.67961 0.18581 0.060653 0.43776 0.13834 -0.48207 -0.56141 -0.25422 -0.52445 0.097003 -0.48925 0.19077 0.21481 1.4969 -0.86665 -3.2846 0.56854 0.41971 1.2294 0.78522 -0.29369 0.63803 -1.5926 -0.20437 1.5306 0.13548 0.50722 0.18742 0.48552 -0.28995 0.19573 0.0046515 0.092879 -0.42444 0.64987 0.52839 0.077908 0.8263 -1.2208 -0.34955 0.49855 -0.64155 -0.72308 0.26566 -1.3643 -0.46364 -0.52048 -1.0525 0.22895 -0.3456 -0.658 -0.16735 0.35158 0.74337 0.26074 0.061104 -0.39079 -0.84557 -0.035432 0.17036\n",
            "'s 0.58854 -0.2025 0.73479 -0.68338 -0.19675 -0.1802 -0.39177 0.34172 -0.60561 0.63816 -0.26695 0.36486 -0.40379 -0.1134 -0.58718 0.2838 0.8025 -0.35303 0.30083 0.078935 0.44416 -0.45906 0.79294 0.50365 0.32805 0.28027 -0.4933 -0.38482 -0.039284 -0.2483 -0.1988 1.1469 0.13228 0.91691 -0.36739 0.89425 0.5426 0.61738 -0.62205 -0.31132 -0.50933 0.23335 1.0826 -0.044637 -0.12767 0.27628 -0.032617 -0.27397 0.77764 -0.50861 0.038307 -0.33679 0.42344 1.2271 -0.53826 -3.2411 0.42626 0.025189 1.3948 0.65085 0.03325 0.37141 0.4044 0.35558 0.98265 -0.61724 0.53901 0.76219 0.30689 0.33065 0.30956 -0.15161 -0.11313 -0.81281 0.6145 -0.44341 -0.19163 -0.089551 -1.5927 0.37405 0.85857 0.54613 -0.31928 0.52598 -1.4802 -0.97931 -0.2939 -0.14724 0.25803 -0.1817 1.0149 0.77649 0.12598 0.54779 -1.0316 0.064599 -0.37523 -0.94475 0.61802 0.39591\n"
          ]
        }
      ],
      "source": [
        "!head glove.6B.100d.txt\n",
        "# !findstr /n . glove.6B.50d.txt  | findstr \"^[0-5]:\" # Use this instead if you are using Windows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4pWMBlnAv7_Z",
        "outputId": "6d2b76a8-cf14-4f00-93ed-96312a6938d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 400000 word vectors.\n"
          ]
        }
      ],
      "source": [
        "glove_dir = ''\n",
        "\n",
        "embeddings_index = {}\n",
        "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'), encoding=\"utf-8\")\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8kk9oVWv7_Z"
      },
      "source": [
        "\n",
        "Now, let's build an embedding matrix that we will be able to load into an `Embedding` layer. It must be a matrix of shape `(max_words, \n",
        "embedding_dim)`, where each entry `i` contains the `embedding_dim`-dimensional vector for the word of index `i` in our reference word index \n",
        "(built during tokenization). Note that the index `0` is not supposed to stand for any word or token -- it's a placeholder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "nbeMJUmpv7_Z"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 100\n",
        "\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if i < max_words:\n",
        "        if embedding_vector is not None:\n",
        "            # Words not found in embedding index will be all-zeros.\n",
        "            embedding_matrix[i] = embedding_vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mRGndrPv7_a"
      },
      "source": [
        "### Define a model\n",
        "\n",
        "We will be using the same model architecture as before:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0p6dO6iv7_a",
        "outputId": "6a35943a-eabc-453c-e56e-d2b821fde2f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     (None, 100, 100)          1000000   \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 10000)             0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 32)                320032    \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,320,065\n",
            "Trainable params: 1,320,065\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow. keras.layers import Embedding, Flatten, Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSo0gd3_v7_a"
      },
      "source": [
        "### Load the GloVe embeddings in the model\n",
        "\n",
        "\n",
        "The `Embedding` layer has a single weight matrix: a 2D float matrix where each entry `i` is the word vector meant to be associated with \n",
        "index `i`. Simple enough. Let's just load the GloVe matrix we prepared into our `Embedding` layer, the first layer in our model.\n",
        "\n",
        "Additionally, we freeze the embedding layer (we set its `trainable` attribute to `False`), so that the pre-trained embeddings are not updated during the training stage. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "0hzeL0Rav7_c"
      },
      "outputs": [],
      "source": [
        "model.layers[0].set_weights([embedding_matrix])\n",
        "model.layers[0].trainable = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qmPks23v7_c"
      },
      "source": [
        "\n",
        "We can now observe that the number of trainable parameters is much smaller:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ircSSesv7_c",
        "outputId": "eced3043-f503-449a-c821-ea3309f648ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     (None, 100, 100)          1000000   \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 10000)             0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 32)                320032    \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,320,065\n",
            "Trainable params: 320,065\n",
            "Non-trainable params: 1,000,000\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCOjTh_7v7_c"
      },
      "source": [
        "### Train and evaluate\n",
        "\n",
        "Let's compile our model and train it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRU0rmR-v7_c",
        "outputId": "527b6b12-da92-465c-84a2-10b08a8cc4dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "7/7 [==============================] - 3s 354ms/step - loss: 1.3340 - acc: 0.5000 - val_loss: 0.6930 - val_acc: 0.5084\n",
            "Epoch 2/10\n",
            "7/7 [==============================] - 1s 224ms/step - loss: 0.5697 - acc: 0.7500 - val_loss: 0.7772 - val_acc: 0.4993\n",
            "Epoch 3/10\n",
            "7/7 [==============================] - 1s 158ms/step - loss: 0.4954 - acc: 0.7250 - val_loss: 0.8917 - val_acc: 0.5075\n",
            "Epoch 4/10\n",
            "7/7 [==============================] - 1s 224ms/step - loss: 0.3200 - acc: 0.9150 - val_loss: 1.2102 - val_acc: 0.5098\n",
            "Epoch 5/10\n",
            "7/7 [==============================] - 1s 225ms/step - loss: 0.2898 - acc: 0.8900 - val_loss: 0.8573 - val_acc: 0.5201\n",
            "Epoch 6/10\n",
            "7/7 [==============================] - 1s 224ms/step - loss: 0.1573 - acc: 0.9900 - val_loss: 1.0284 - val_acc: 0.5195\n",
            "Epoch 7/10\n",
            "7/7 [==============================] - 1s 152ms/step - loss: 0.1854 - acc: 0.9600 - val_loss: 0.9794 - val_acc: 0.5090\n",
            "Epoch 8/10\n",
            "7/7 [==============================] - 1s 142ms/step - loss: 0.1011 - acc: 1.0000 - val_loss: 0.7643 - val_acc: 0.5635\n",
            "Epoch 9/10\n",
            "7/7 [==============================] - 1s 159ms/step - loss: 0.0418 - acc: 1.0000 - val_loss: 0.8034 - val_acc: 0.5647\n",
            "Epoch 10/10\n",
            "7/7 [==============================] - 1s 224ms/step - loss: 0.0419 - acc: 0.9900 - val_loss: 3.0338 - val_acc: 0.5050\n"
          ]
        }
      ],
      "source": [
        "model.compile(optimizer='rmsprop',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['acc'])\n",
        "history = model.fit(x_train, y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=32,\n",
        "                    validation_data=(x_val, y_val))\n",
        "model.save_weights('pre_trained_glove_model.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cMp2SXnv7_c"
      },
      "source": [
        "Let's plot its performance over time:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "JCjYPX7wv7_d",
        "outputId": "bfbb77a3-e86b-483b-fdda-c67bf0fdec50"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.subplot(121)\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "#plt.figure()\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l56lZMXuv7_d"
      },
      "source": [
        "\n",
        "The model quickly starts overfitting, unsurprisingly given the small number of training samples. Validation accuracy has high variance for \n",
        "the same reason, but seems to reach high 50s.\n",
        "\n",
        "Note that your mileage may vary: since we have so few training samples, performance is heavily dependent on which exact 200 samples we \n",
        "picked, and we picked them at random. If it worked really poorly for you, try picking a different random set of 200 samples, just for the \n",
        "sake of the exercise (in real life you don't get to pick your training data).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Facj2I6sv7_d"
      },
      "source": [
        "We can also try to train the same model without loading the pre-trained word embeddings and without freezing the embedding layer. In that \n",
        "case, we would be learning a task-specific embedding of our input tokens, which is generally more powerful than pre-trained word embeddings \n",
        "when lots of data is available. However, in our case, we have only 200 training samples. Let's try it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYU_oNIPv7_e",
        "outputId": "d1a028ff-51c9-4d90-8a12-0bf726a9246e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_3 (Embedding)     (None, 100, 100)          1000000   \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 10000)             0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 32)                320032    \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,320,065\n",
            "Trainable params: 1,320,065\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "7/7 [==============================] - 2s 207ms/step - loss: 0.6895 - acc: 0.4900 - val_loss: 0.6955 - val_acc: 0.5190\n",
            "Epoch 2/10\n",
            "7/7 [==============================] - 1s 168ms/step - loss: 0.4739 - acc: 0.9700 - val_loss: 0.7121 - val_acc: 0.5193\n",
            "Epoch 3/10\n",
            "7/7 [==============================] - 1s 232ms/step - loss: 0.2525 - acc: 0.9850 - val_loss: 0.7176 - val_acc: 0.5214\n",
            "Epoch 4/10\n",
            "7/7 [==============================] - 1s 231ms/step - loss: 0.1098 - acc: 0.9950 - val_loss: 0.7377 - val_acc: 0.5261\n",
            "Epoch 5/10\n",
            "7/7 [==============================] - 1s 230ms/step - loss: 0.0528 - acc: 0.9950 - val_loss: 0.7448 - val_acc: 0.5194\n",
            "Epoch 6/10\n",
            "7/7 [==============================] - 1s 230ms/step - loss: 0.0276 - acc: 1.0000 - val_loss: 0.7531 - val_acc: 0.5255\n",
            "Epoch 7/10\n",
            "7/7 [==============================] - 1s 231ms/step - loss: 0.0152 - acc: 1.0000 - val_loss: 0.7578 - val_acc: 0.5290\n",
            "Epoch 8/10\n",
            "7/7 [==============================] - 1s 127ms/step - loss: 0.0087 - acc: 1.0000 - val_loss: 0.7702 - val_acc: 0.5263\n",
            "Epoch 9/10\n",
            "7/7 [==============================] - 1s 228ms/step - loss: 0.0052 - acc: 1.0000 - val_loss: 0.7876 - val_acc: 0.5278\n",
            "Epoch 10/10\n",
            "7/7 [==============================] - 1s 124ms/step - loss: 0.0032 - acc: 1.0000 - val_loss: 0.8059 - val_acc: 0.5234\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Flatten, Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['acc'])\n",
        "history = model.fit(x_train, y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=32,\n",
        "                    validation_data=(x_val, y_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "A2QSsUDxv7_f",
        "outputId": "53b1aa1b-dcd2-4d29-a73f-6d900041b119"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.subplot(121)\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "#plt.figure()\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5DF-Rqjv7_r"
      },
      "source": [
        "\n",
        "Validation accuracy stalls in the low 50s. So in our case, pre-trained word embeddings does outperform jointly learned embeddings. If you \n",
        "increase the number of training samples, this will quickly stop being the case -- try it as an exercise.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPhyeQu9v7_r"
      },
      "source": [
        "Finally, let's evaluate the model on the test data. First, we will need to tokenize the test data. Remember that we have created a mapping of words to indices. We now reuse this mapping by calling to `texts_to_sequences`. **You should not call to `fit_on_texts` now!** (why not?)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "38MrKvatv7_r"
      },
      "outputs": [],
      "source": [
        "test_dir = os.path.join(imdb_dir, 'test')\n",
        "\n",
        "labels = []\n",
        "texts = []\n",
        "\n",
        "for label_type in ['neg', 'pos']:\n",
        "    dir_name = os.path.join(test_dir, label_type)\n",
        "    for fname in sorted(os.listdir(dir_name)):\n",
        "        if fname[-4:] == '.txt':\n",
        "            f = open(os.path.join(dir_name, fname), encoding=\"utf-8\")\n",
        "            texts.append(f.read())\n",
        "            f.close()\n",
        "            if label_type == 'neg':\n",
        "                labels.append(0)\n",
        "            else:\n",
        "                labels.append(1)\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "x_test = pad_sequences(sequences, maxlen=maxlen)\n",
        "y_test = np.asarray(labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_A6jx0yv7_r"
      },
      "source": [
        "And let's load and evaluate the first model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jh3i4MnCv7_s",
        "outputId": "58127cf3-b9a5-49c5-99d3-c0a571c8c4e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 3s 4ms/step - loss: 3.0383 - acc: 0.5032\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3.038332462310791, 0.5031999945640564]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "model.load_weights('pre_trained_glove_model.h5')\n",
        "model.evaluate(x_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJUOEWJev7_s"
      },
      "source": [
        "We get an appalling test accuracy of 50%. Working with just a handful of training samples is hard!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w1-M-IlIv7_s"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Slideshow",
    "interpreter": {
      "hash": "a7b63e7410c98f344f02082f10d790581d1dba1eeb1c8fe30f342f6109f0429e"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "W05L1-1-WordEmbeddings.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}