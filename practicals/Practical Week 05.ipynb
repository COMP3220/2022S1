{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OSQVr3b7v_Rg"
   },
   "source": [
    "# An End-to-End Text Classification System\n",
    "\n",
    "In this workshop you will implement a text classification system from scratch. This means that we will not rely on Keras' convenient data sets. These data sets are pre-processed and it will be useful if you know how to tokenise and find the word indices of text collections not provided by Keras.\n",
    "\n",
    "The task will be to classify questions. To run this task we advice that you use [Google Colaboratory](https://colab.research.google.com) (also called Google Colab), which is a cloud solution to run Jupyter notebooks. The demonstrator will show how to use Google Colab. For additional information and to practice with the use of notebooks in Google Colab, you can also follow this link:\n",
    "\n",
    "* [Welcome notebook and link to additional resources](https://colab.research.google.com/notebooks/welcome.ipynb)\n",
    "\n",
    "## Question Classification\n",
    "\n",
    "NLTK has a corpus of questions and their question types according to a particular classification scheme (e.g. DESC refers to a question expecting a descriptive answer, such as one starting with \"How\"; HUM refers to a question expecting an answer referring to a human). Below is an example of use of the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "HdoJ387Vv_Rq",
    "outputId": "898327fe-98d1-46b2-f003-2d82149b474f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package qc to /home/diego/nltk_data...\n",
      "[nltk_data]   Package qc is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"qc\")\n",
    "from nltk.corpus import qc\n",
    "train = qc.tuples(\"train.txt\")\n",
    "test = qc.tuples(\"test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "U78gbdkzv_SM",
    "outputId": "fe8d9865-b5d7-4716-bbae-cde13050e644"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('DESC:manner', 'How did serfdom develop in and then leave Russia ?'),\n",
       " ('ENTY:cremat', 'What films featured the character Popeye Doyle ?'),\n",
       " ('DESC:manner', \"How can I find a list of celebrities ' real names ?\")]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "dUCCU6awv_Si",
    "outputId": "e46ecb4c-61ae-4214-9d7f-6de2559930d6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NUM:dist', 'How far is it from Denver to Aspen ?'),\n",
       " ('LOC:city', 'What county is Modesto , California in ?'),\n",
       " ('HUM:desc', 'Who was Galileo ?')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WCOTaE2Yv_S-"
   },
   "source": [
    "### Exercise: Find all question types\n",
    "Write Python code that lists all the possible question types of the training set (**remember: for data exploration, never look at the test set**).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yn7EUUwUv_TE"
   },
   "outputs": [],
   "source": [
    "qtypes = # ... write your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 885
    },
    "colab_type": "code",
    "id": "AvETLOhxv_TW",
    "outputId": "cc804643-b210-4622-8dd6-6e910f21a5ae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ENTY:religion',\n",
       " 'DESC:desc',\n",
       " 'ENTY:product',\n",
       " 'ABBR:exp',\n",
       " 'ENTY:plant',\n",
       " 'NUM:date',\n",
       " 'ENTY:dismed',\n",
       " 'LOC:state',\n",
       " 'ENTY:event',\n",
       " 'ENTY:currency',\n",
       " 'NUM:volsize',\n",
       " 'DESC:reason',\n",
       " 'ENTY:techmeth',\n",
       " 'ENTY:sport',\n",
       " 'ENTY:symbol',\n",
       " 'ENTY:other',\n",
       " 'HUM:title',\n",
       " 'NUM:other',\n",
       " 'NUM:dist',\n",
       " 'DESC:def',\n",
       " 'LOC:other',\n",
       " 'ABBR:abb',\n",
       " 'DESC:manner',\n",
       " 'ENTY:veh',\n",
       " 'NUM:ord',\n",
       " 'ENTY:letter',\n",
       " 'ENTY:food',\n",
       " 'HUM:desc',\n",
       " 'NUM:speed',\n",
       " 'LOC:mount',\n",
       " 'ENTY:substance',\n",
       " 'ENTY:termeq',\n",
       " 'NUM:weight',\n",
       " 'HUM:ind',\n",
       " 'ENTY:cremat',\n",
       " 'LOC:city',\n",
       " 'NUM:period',\n",
       " 'NUM:perc',\n",
       " 'NUM:temp',\n",
       " 'ENTY:animal',\n",
       " 'ENTY:body',\n",
       " 'NUM:code',\n",
       " 'ENTY:instru',\n",
       " 'HUM:gr',\n",
       " 'NUM:money',\n",
       " 'NUM:count',\n",
       " 'ENTY:lang',\n",
       " 'ENTY:word',\n",
       " 'ENTY:color',\n",
       " 'LOC:country']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "inQh0Nrjv_Tp"
   },
   "source": [
    "### Exercise: Find all general types\n",
    "\n",
    "The question types have two parts. The first part describes a general type, and the second part defines a subtype. For example, the question type `DESC:manner` belongs to the general `DESC` type and within that type to the `manner` subtype. Let's focus on the general types only. Write Python code that lists all the possible general types (there are 6 of them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "C4pjpWV9v_Tt",
    "outputId": "cc7df776-c9df-412e-b77a-88a93ba50f6f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ABBR', 'NUM', 'HUM', 'ENTY', 'LOC', 'DESC']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "general_types = # ... write your answer here\n",
    "general_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GnMXsRBMv_T-"
   },
   "source": [
    "### Exercise: Partition the data\n",
    "There is a train and test data, but for this exercise we want to have a partition into train, dev-test, and test. In this exercise, combine all data into one array and do a 3-way partition into train, dev-test, and test. Make sure that you shuffle the data prior to doing the partition. Also, make sure that you only use the general label types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "qq6JJ-EBv_UE",
    "outputId": "66d0661d-191c-4cf4-fe4c-087b194d5e8a"
   },
   "outputs": [],
   "source": [
    "# ... write your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EWQx8J2tv_Ua"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(1234)\n",
    "random.shuffle(qdata)\n",
    "threshold1 = int(len(qdata)*.6)\n",
    "threshold2 = int(len(qdata)*.8)\n",
    "q_train = qdata[:threshold1]\n",
    "q_devtest = qdata[threshold1:threshold2]\n",
    "q_test = qdata[threshold2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Iri6AvfYv_Uj"
   },
   "source": [
    "### Exercise: Tokenise the data\n",
    "\n",
    "Use Keras' tokeniser to tokenise all the data. For this exercise we will use only the 100 most frequent words in the training set (since you aren't supposed to use the dev-test or test sets to extract features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3Jbf0L_Qv_Un"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# write your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UE2ecEF2v_U2"
   },
   "outputs": [],
   "source": [
    "indices_train = # ... write your code here\n",
    "indices_devtest = # ... write your code here\n",
    "indices_test = # ... write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "epuXbJoyv_VA"
   },
   "source": [
    "### Exercise: Vectorize the data\n",
    "The following code shows the distribution of lengths of my training data (could be different in your training data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "colab_type": "code",
    "id": "ZEaD0ouFv_VE",
    "outputId": "ebc05931-72ee-450d-8d78-bd5602d3fc53"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  43., 1001., 1327.,  815.,  169.,  162.,   43.,    7.,    2.,\n",
       "           2.]),\n",
       " array([ 0. ,  1.8,  3.6,  5.4,  7.2,  9. , 10.8, 12.6, 14.4, 16.2, 18. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "plt.hist([len(d) for d in indices_train])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t34OuB9vv_VQ"
   },
   "source": [
    "The histogram shows that the longest question in the training data has 18 word indices, but by far most of the questions have at least 10. Based on this, use Keras' `pad_sequences` to vectorize the questions into sequences of 10 word indices. The default will be to truncate the beginning, but we want to truncate the end (since the first words of a question are often very important to determine the question type). For this you can use the option `truncating='post'`: https://keras.io/preprocessing/sequence/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "15jgaUwpv_VS"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "maxlen = 10\n",
    "x_train = pad_sequences(indices_train, maxlen=maxlen, truncating='post')\n",
    "x_devtest = pad_sequences(indices_devtest, maxlen=maxlen, truncating='post')\n",
    "x_test = pad_sequences(indices_test, maxlen=maxlen, truncating='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qn1vV1Kov_Vd"
   },
   "source": [
    "### Exercise: Vectorise the labels\n",
    "Convert the labels to one-hot encoding. If you use Keras' `to_categorical`, you will first need to convert the labels to integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OfRBWqadv_Vh"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "y_train = # ... write your code here\n",
    "y_devtest = # ... write your code here\n",
    "y_test = # ... write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iwNCab0Kv_Vs"
   },
   "source": [
    "### Exercise: Define the model\n",
    "\n",
    "Define a model for classification. For this model, use a feedforward architecture with an embedding layer of size 20, a layer that computes the average of word embeddings (use `GlobalAveragePooling1D`), a hidden layer of 16 units, and `relu` activation. You need to determine the size and activation of the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "colab_type": "code",
    "id": "pBcT2-btv_Vu",
    "outputId": "a53ea9fe-1e35-49f5-c8f6-5b34e06767ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 10, 20)            2000      \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                336       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6)                 102       \n",
      "=================================================================\n",
      "Total params: 2,438\n",
      "Trainable params: 2,438\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-17 16:01:25.273555: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-03-17 16:01:25.273822: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-03-17 16:01:25.274223: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, GlobalAveragePooling1D\n",
    "\n",
    "embedding_dim = 20\n",
    "\n",
    "# write your code here\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CQxpGsUcv_V5"
   },
   "source": [
    "### Exercise: Train and evaluate\n",
    "Train your model. In the process you need to determine the optimal number of epochs. Then answer the following questions:\n",
    "1. What was the optimal number of epochs and how did you determine this?\n",
    "2. Is the system overfitting? Justify your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "8DfIKpyYv_V7",
    "outputId": "4aa48185-a265-46b0-b9bd-b2872662ab15"
   },
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AbK8NkhRv_Wb"
   },
   "source": [
    "### Optional Exercise: Data exploration\n",
    "Plot the distribution of labels in the training data and compare with the distribution of labels in the devtest data. Plot also the distribution of predictions in the devtest data. What can you learn from this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "ZYWeFIznv_Wg",
    "outputId": "8c2291ac-a1ab-4d40-aa89-9b902c15b312"
   },
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QOUTCrdqv_Xi"
   },
   "source": [
    "### Optional Exercise: Improve your system\n",
    "\n",
    "Try the following options:\n",
    "\n",
    "1. Use pre-trained word embeddings\n",
    "2. Use recurrent neural networks.\n",
    "\n",
    "Feel free to try each option separately and in combination, and compare the results. Feel also free to try with other variants of the initial architecture, such as:\n",
    "\n",
    "1. Introducing more hidden layers.\n",
    "2. Changing the size of embeddings.\n",
    "3. Changing the number of units in the hidden layer(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6Q-1GtYOv_Xl"
   },
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "W05 Workshop-Solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
