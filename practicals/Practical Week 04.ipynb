{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop Week 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for Name Gender Classification\n",
    "\n",
    "We have already seen the following code for partitioning the data of name gender classification and feature extraction. The code is changed slightly so that the labels are numerical (0 for male, 1 for female). This is the format required for Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package names to\n",
      "[nltk_data]     C:\\Users\\K\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package names is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "nltk.download('names')\n",
    "from nltk.corpus import names\n",
    "m = names.words('male.txt')\n",
    "f = names.words('female.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(1234) # Set the random seed to allow replicability\n",
    "names = ([(name,0) for name in m] +\n",
    "         [(name,1) for name in f])\n",
    "random.shuffle(names)\n",
    "train_names = names[1000:]\n",
    "devtest_names = names[500:1000]\n",
    "test_names = names[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_character(c):\n",
    "    alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    result = [0]*(len(alphabet)+1)\n",
    "    i = alphabet.find(c.lower())\n",
    "    if i >= 0:\n",
    "        result[i] = 1\n",
    "    else:\n",
    "        result[len(alphabet)] = 1 # character is out of the alphabet\n",
    "    return result\n",
    "\n",
    "def gender_features_n(word, n=2):\n",
    "    \"Return the one-hot encodings of the last n characters\"\n",
    "    features = []\n",
    "    for i in range(n):\n",
    "        if i < len(word):\n",
    "            features = one_hot_character(word[-i-1]) + features\n",
    "        else:\n",
    "            features = one_hot_character(' ') + features\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gender_features_n(\"Mary\", n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's determine the number of features so that we can use this information when we design the neural network\n",
    "len(gender_features_n(\"Mary\", n=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Simple Neural Network\n",
    "Design a simple neural network that has 54 input cells (that's the number of gender features for $n=2$, as we have seen above), and one output cell (without a hidden layer). The output cell will be used to classify the name between male (output=0) and female (output=1). This is therefore an instance of **binary classification**. Pay attention to the right activation function! This simple model, without hidden layers, is equivalent to a **logistic regression** classifier. The model summary should look like this:\n",
    "\n",
    "```\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "dense_2 (Dense)              (None, 1)                 55        \n",
    "=================================================================\n",
    "Total params: 55\n",
    "Trainable params: 55\n",
    "Non-trainable params: 0\n",
    "```\n",
    "\n",
    "\n",
    "Compile the model and provide the right loss function. Use `'rmsprop'` as the optimiser, and include `'accuracy'` as an evaluation metric. \n",
    "Run the network **for 100 epochs** using batch size of 100, and observe the results. \n",
    "\n",
    "Answer the following questions:\n",
    "\n",
    "1. What is the best result on the validation set?\n",
    "2. At the epoch with best result on the validation set, what is the result on the training set?\n",
    "3. Is the system overfitting? Justify your answer.\n",
    "4. Do we really need 100 epochs? Do we need more than 100 epochs? would the system run better with less epochs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.experimental.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.array([gender_features_n(name,n=2)for name, label in train_names])\n",
    "train_labels = np.array([label for name, label in train_names])\n",
    "\n",
    "devtest_data = np.array([gender_features_n(name,n=2)for name, label in devtest_names])\n",
    "devtest_labels = np.array([label for name, label in devtest_names])\n",
    "\n",
    "test_data = np.array([gender_features_n(name,n=2)for name, label in test_names])\n",
    "test_labels = np.array([name for name, label in test_names])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_5 (Dense)             (None, 1)                 55        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 55\n",
      "Trainable params: 55\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.7256 - acc: 0.4751 - val_loss: 0.7198 - val_acc: 0.4920\n",
      "Epoch 2/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.7156 - acc: 0.4873 - val_loss: 0.7113 - val_acc: 0.5000\n",
      "Epoch 3/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.7073 - acc: 0.4948 - val_loss: 0.7034 - val_acc: 0.5000\n",
      "Epoch 4/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.6995 - acc: 0.5042 - val_loss: 0.6959 - val_acc: 0.5280\n",
      "Epoch 5/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.6920 - acc: 0.5156 - val_loss: 0.6886 - val_acc: 0.5300\n",
      "Epoch 6/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.6847 - acc: 0.5232 - val_loss: 0.6816 - val_acc: 0.5400\n",
      "Epoch 7/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.6777 - acc: 0.5413 - val_loss: 0.6749 - val_acc: 0.5640\n",
      "Epoch 8/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.6710 - acc: 0.5763 - val_loss: 0.6684 - val_acc: 0.5780\n",
      "Epoch 9/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.6644 - acc: 0.6118 - val_loss: 0.6621 - val_acc: 0.6420\n",
      "Epoch 10/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.6581 - acc: 0.6524 - val_loss: 0.6560 - val_acc: 0.6600\n",
      "Epoch 11/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.6520 - acc: 0.6727 - val_loss: 0.6503 - val_acc: 0.6680\n",
      "Epoch 12/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.6461 - acc: 0.7006 - val_loss: 0.6447 - val_acc: 0.7060\n",
      "Epoch 13/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.6404 - acc: 0.7092 - val_loss: 0.6393 - val_acc: 0.7060\n",
      "Epoch 14/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.6349 - acc: 0.7101 - val_loss: 0.6341 - val_acc: 0.7060\n",
      "Epoch 15/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.6297 - acc: 0.7103 - val_loss: 0.6293 - val_acc: 0.6980\n",
      "Epoch 16/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.6246 - acc: 0.7134 - val_loss: 0.6245 - val_acc: 0.7020\n",
      "Epoch 17/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.6197 - acc: 0.7150 - val_loss: 0.6200 - val_acc: 0.7020\n",
      "Epoch 18/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.6151 - acc: 0.7153 - val_loss: 0.6157 - val_acc: 0.7000\n",
      "Epoch 19/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.6106 - acc: 0.7173 - val_loss: 0.6116 - val_acc: 0.6980\n",
      "Epoch 20/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.6063 - acc: 0.7175 - val_loss: 0.6076 - val_acc: 0.6980\n",
      "Epoch 21/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.6020 - acc: 0.7172 - val_loss: 0.6038 - val_acc: 0.7020\n",
      "Epoch 22/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5980 - acc: 0.7163 - val_loss: 0.6002 - val_acc: 0.7000\n",
      "Epoch 23/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5942 - acc: 0.7141 - val_loss: 0.5967 - val_acc: 0.7000\n",
      "Epoch 24/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5905 - acc: 0.7144 - val_loss: 0.5935 - val_acc: 0.7000\n",
      "Epoch 25/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5870 - acc: 0.7141 - val_loss: 0.5903 - val_acc: 0.7000\n",
      "Epoch 26/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5835 - acc: 0.7141 - val_loss: 0.5873 - val_acc: 0.7000\n",
      "Epoch 27/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5802 - acc: 0.7140 - val_loss: 0.5844 - val_acc: 0.7000\n",
      "Epoch 28/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5771 - acc: 0.7199 - val_loss: 0.5816 - val_acc: 0.7100\n",
      "Epoch 29/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5740 - acc: 0.7199 - val_loss: 0.5791 - val_acc: 0.7000\n",
      "Epoch 30/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5711 - acc: 0.7196 - val_loss: 0.5765 - val_acc: 0.6980\n",
      "Epoch 31/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5682 - acc: 0.7185 - val_loss: 0.5741 - val_acc: 0.6980\n",
      "Epoch 32/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5655 - acc: 0.7176 - val_loss: 0.5718 - val_acc: 0.7000\n",
      "Epoch 33/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5628 - acc: 0.7176 - val_loss: 0.5695 - val_acc: 0.7000\n",
      "Epoch 34/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5601 - acc: 0.7232 - val_loss: 0.5673 - val_acc: 0.7060\n",
      "Epoch 35/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5576 - acc: 0.7252 - val_loss: 0.5652 - val_acc: 0.7060\n",
      "Epoch 36/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5551 - acc: 0.7254 - val_loss: 0.5631 - val_acc: 0.7060\n",
      "Epoch 37/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5527 - acc: 0.7254 - val_loss: 0.5612 - val_acc: 0.7060\n",
      "Epoch 38/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5504 - acc: 0.7264 - val_loss: 0.5593 - val_acc: 0.7140\n",
      "Epoch 39/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5482 - acc: 0.7340 - val_loss: 0.5574 - val_acc: 0.7160\n",
      "Epoch 40/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5460 - acc: 0.7365 - val_loss: 0.5556 - val_acc: 0.7140\n",
      "Epoch 41/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5438 - acc: 0.7382 - val_loss: 0.5539 - val_acc: 0.7140\n",
      "Epoch 42/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5418 - acc: 0.7385 - val_loss: 0.5522 - val_acc: 0.7140\n",
      "Epoch 43/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5397 - acc: 0.7388 - val_loss: 0.5506 - val_acc: 0.7100\n",
      "Epoch 44/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5378 - acc: 0.7404 - val_loss: 0.5490 - val_acc: 0.7140\n",
      "Epoch 45/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5358 - acc: 0.7432 - val_loss: 0.5474 - val_acc: 0.7140\n",
      "Epoch 46/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5340 - acc: 0.7431 - val_loss: 0.5459 - val_acc: 0.7140\n",
      "Epoch 47/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5321 - acc: 0.7431 - val_loss: 0.5445 - val_acc: 0.7140\n",
      "Epoch 48/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5303 - acc: 0.7437 - val_loss: 0.5430 - val_acc: 0.7140\n",
      "Epoch 49/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5286 - acc: 0.7438 - val_loss: 0.5417 - val_acc: 0.7140\n",
      "Epoch 50/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5269 - acc: 0.7490 - val_loss: 0.5403 - val_acc: 0.7220\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'acc', 'val_loss', 'val_acc'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras import optimizers\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "# Write your model here\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(1, activation='sigmoid', input_shape = (54,)))\n",
    "network.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy', metrics = ['acc'])\n",
    "network.summary()\n",
    "history = network.fit(train_data, train_labels, epochs= 50, batch_size =512, validation_data=(devtest_data, devtest_labels))\n",
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 1)                 55        \n",
      "=================================================================\n",
      "Total params: 55\n",
      "Trainable params: 55\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(write additional code for the partition of the data, your experiments, and your analysis. Write the answers to the questions.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: A Deeper Network\n",
    "Experiment with a network that has one hidden dense layer with a `'relu'` activation. The resulting system is no longer a logistic regression classifier, it's something more complex. Try the following sizes in the hidden layer:\n",
    "\n",
    "* 5, 7, 10\n",
    "\n",
    "Answer the following questions:\n",
    "\n",
    "1. Which system performed best on the dev-test set?\n",
    "2. Would you add more or less cells in the hidden layer? Justify your answer.\n",
    "3. Is this system better than the simpler system of the previous exercise? Justify your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional: Deep Learning with the Movie Review Corpus\n",
    "The notebook [W04L1-2-MovieReviews.ipynb](../lectures/W04L1-2-MovieReviews.ipynb) has several questions at the end, repeated below. Try to answer these, and indeed try other variants!\n",
    "\n",
    "* We were using 2 hidden layers. Try to use 1 or 3 hidden layers and see how it affects validation and test accuracy.\n",
    "* Try to use layers with more hidden units or less hidden units: 32 units, 64 units...\n",
    "* Try to use the `mse` loss function instead of `binary_crossentropy`.\n",
    "* Try to use the `tanh` activation (an activation that was popular in the early days of neural networks) instead of `relu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
